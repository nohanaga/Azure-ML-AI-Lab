{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module1: Face API のテスト\n",
    "\n",
    "このノートブックでは、Face API を利用して、顔写真から顔の属性情報を取得します。\n",
    "属性情報の推定に使用する画像ファイルはWeb上にあるものと、ローカルマシン上にあるものの両方を使用してテストします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "### 必要なライブラリをインストールする\n",
    "\n",
    "各セルを1つずつ実行して、必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install azure-cognitiveservices-vision-face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "互換性に関連するエラーは無視してください。 最後にこのメッセージが表示されている限り正常にインストールされています: Successfully installed azure-cognitiveservices-vision-face もしくは Requirement already satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Face APIを使用したテスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下のセルで、Face APIのキーを指定してセルを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サブスクリプションキーを記入します\n",
    "subscription_key = '' #\"<your_face_api_key>\"\n",
    "assert subscription_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルを実行して、Face APIのエンドポイントを指定します。\n",
    "\n",
    "Face API サービスのインスタンスの Azure Portal からコピーしたエンドポイント値と一致するように、以下の face_endpoint の値を必ず更新してください。`値がスラッシュ（/）で終わっていることを確認してください`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face API のエンドポイントを記入します\n",
    "face_endpoint = '' #\"<your_face_api_endpoint>\"\n",
    "\n",
    "# 認証済みの FaceClient オブジェクトを作成します。\n",
    "face_client = FaceClient(face_endpoint, CognitiveServicesCredentials(subscription_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face API の実行\n",
    "\n",
    "まずはWeb上に既定で用意されている顔画像から顔属性を抽出してみましょう。\n",
    "\n",
    "[FaceAttributes](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithstream#faceattributes) をパラメータに指定することで、取得する属性を指定することができます。\n",
    "その中で、Emotion プロパティは、0から1までの信頼度形式で感情を表し、anger, contempt, disgust, fear, happiness, neutral, sadness, surprise の8種類が推定可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 画像ファイルのあるURLを指定\n",
    "single_face_image_url = 'https://raw.githubusercontent.com/Microsoft/Cognitive-Face-Windows/master/Data/detection1.jpg'\n",
    "\n",
    "# API コールで返したい属性、FaceAttributeType enum（文字列形式）のリスト\n",
    "face_attributes = ['age', 'gender', 'headPose', 'smile', 'facialHair', 'glasses', 'emotion']\n",
    "\n",
    "# 顔検出実行\n",
    "detected_faces = face_client.face.detect_with_url(url=single_face_image_url, return_face_attributes=face_attributes)\n",
    "if not detected_faces:\n",
    "    raise Exception('画像から顔が検出されませんでした {}'.format(single_image_name))\n",
    "\n",
    "'''\n",
    "検出された顔を属性とバウンディングボックスで表示する\n",
    "'''\n",
    "# Face ID は、他の画像から検出された顔と比較するために使用される。\n",
    "for face in detected_faces:\n",
    "    print()\n",
    "    print('Detected face ID from', ':')\n",
    "    # ID of detected face\n",
    "    print(face.face_id)\n",
    "    # Show all facial attributes from the results\n",
    "    print()\n",
    "    print('Facial attributes detected:')\n",
    "    print('Age:', face.face_attributes.age)\n",
    "    print('Gender:', face.face_attributes.gender)\n",
    "    print('Head pose:', face.face_attributes.head_pose)\n",
    "    print('Smile:', face.face_attributes.smile)\n",
    "    print('Facial hair:', face.face_attributes.facial_hair)\n",
    "    print('Glasses:', face.face_attributes.glasses)\n",
    "    print('Emotion:', face.face_attributes.emotion)\n",
    "    print()\n",
    "\n",
    "    \n",
    "# 幅と高さを矩形内の座標に変換する\n",
    "def getRectangle(faceDictionary):\n",
    "    rect = faceDictionary.face_rectangle\n",
    "    left = rect.left\n",
    "    top = rect.top\n",
    "    right = left + rect.width\n",
    "    bottom = top + rect.height\n",
    "    return ((left, top), (right, bottom))\n",
    "\n",
    "# URLから画像をダウンロード\n",
    "response = requests.get(single_face_image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "draw = ImageDraw.Draw(img)\n",
    "for face in detected_faces:\n",
    "    draw.rectangle(getRectangle(face), outline='red', width=3)\n",
    "\n",
    "# 画像を表示\n",
    "plt.figure(figsize=(8.0, 6.0))\n",
    "#PIL image to matplotlib image\n",
    "im_list = np.asarray(img)\n",
    "plt.imshow(im_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ストリームで検出\n",
    "\n",
    "今度はローカルマシンから画像をストリームとしてアップロードする方式で顔検出を行います。\n",
    "\n",
    "[Detect With Stream](https://docs.microsoft.com/rest/api/cognitiveservices/face/face/detectwithstream) を使用します。\n",
    "\n",
    "emotions フォルダには、サンプルの顔写真が 01.jpg~12jpg まで用意してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルマシンの画像ファイルパスを指定\n",
    "image_path = r\"emotions/01.jpg\"\n",
    "image = open(image_path, 'r+b')\n",
    "\n",
    "face_ids = []\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image, return_face_attributes=face_attributes)\n",
    "\n",
    "if not detected_faces:\n",
    "    raise Exception('画像から顔が検出されませんでした {}'.format(image_path))\n",
    "    \n",
    "for face in detected_faces:\n",
    "    face_ids.append(face.face_id)\n",
    "\n",
    "'''\n",
    "検出された顔を属性とバウンディングボックスで表示する\n",
    "'''\n",
    "# Face ID は、他の画像から検出された顔と比較するために使用される。\n",
    "for face in detected_faces:\n",
    "    print()\n",
    "    print('Detected face ID from', ':')\n",
    "    # ID of detected face\n",
    "    print(face.face_id)\n",
    "    # Show all facial attributes from the results\n",
    "    print()\n",
    "    print('Facial attributes detected:')\n",
    "    print('Age:', face.face_attributes.age)\n",
    "    print('Gender:', face.face_attributes.gender)\n",
    "    print('Head pose:', face.face_attributes.head_pose)\n",
    "    print('Smile:', face.face_attributes.smile)\n",
    "    print('Facial hair:', face.face_attributes.facial_hair)\n",
    "    print('Glasses:', face.face_attributes.glasses)\n",
    "    print('Emotion:', face.face_attributes.emotion)\n",
    "    print()\n",
    "    \n",
    "# open the image\n",
    "img = Image.open(image_path)\n",
    "\n",
    "draw = ImageDraw.Draw(img)\n",
    "for face in detected_faces:\n",
    "    draw.rectangle(getRectangle(face), outline='red', width=3)\n",
    "\n",
    "# 画像を表示\n",
    "plt.figure(figsize=(8.0, 6.0))\n",
    "#PIL image to matplotlib image\n",
    "im_list = np.asarray(img)\n",
    "plt.imshow(im_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
